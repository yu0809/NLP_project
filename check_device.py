"""
æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§è„šæœ¬
"""
import torch

print("=" * 60)
print("è®¾å¤‡æ£€æµ‹")
print("=" * 60)

print(f"\nPyTorch ç‰ˆæœ¬: {torch.__version__}")

print(f"\nCUDA (NVIDIA GPU):")
print(f"  å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"  è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
    print(f"  å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
    print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")

print(f"\nMPS (Apple Silicon GPU):")
if hasattr(torch.backends, 'mps'):
    mps_available = torch.backends.mps.is_available()
    print(f"  å¯ç”¨: {mps_available}")
    if mps_available:
        print(f"  âœ“ Mac M ç³»åˆ—èŠ¯ç‰‡ GPU åŠ é€Ÿå¯ç”¨ï¼")
else:
    print(f"  ä¸å¯ç”¨ (PyTorch ç‰ˆæœ¬å¯èƒ½è¿‡ä½ï¼Œéœ€è¦ >= 1.12)")

print(f"\nCPU:")
print(f"  å¯ç”¨: True")

print("\n" + "=" * 60)
print("æ¨èè®¾å¤‡:")
print("=" * 60)

if torch.cuda.is_available():
    print("âœ“ ä½¿ç”¨ CUDA (NVIDIA GPU)")
    device = "cuda"
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    print("âœ“ ä½¿ç”¨ MPS (Apple Silicon GPU)")
    device = "mps"
else:
    print("âš  ä½¿ç”¨ CPU (æ—  GPU åŠ é€Ÿ)")
    device = "cpu"

print(f"\nå½“å‰æ¨èè®¾å¤‡: {device}")

# æµ‹è¯•è®¾å¤‡
print("\n" + "=" * 60)
print("è®¾å¤‡æµ‹è¯•")
print("=" * 60)

try:
    test_device = torch.device(device)
    x = torch.randn(3, 3).to(test_device)
    y = x * 2
    print(f"âœ“ è®¾å¤‡ {device} æµ‹è¯•æˆåŠŸ")
    print(f"  æµ‹è¯•å¼ é‡å½¢çŠ¶: {x.shape}")
except Exception as e:
    print(f"âŒ è®¾å¤‡ {device} æµ‹è¯•å¤±è´¥: {str(e)}")
    print("  å°†å›é€€åˆ° CPU")
    device = "cpu"

print("\n" + "=" * 60)
print("æ€»ç»“")
print("=" * 60)
print(f"è¿è¡Œè„šæœ¬æ—¶å°†ä½¿ç”¨: {device}")
if device == "mps":
    print("\nğŸ’¡ Mac M ç³»åˆ—èŠ¯ç‰‡å°†ä½¿ç”¨ GPU åŠ é€Ÿï¼Œæ€§èƒ½æ¯” CPU å¿« 3-5 å€ï¼")
elif device == "cuda":
    print("\nğŸ’¡ å°†ä½¿ç”¨ NVIDIA GPU åŠ é€Ÿ")
else:
    print("\nâš ï¸  å°†ä½¿ç”¨ CPUï¼Œé€Ÿåº¦è¾ƒæ…¢")

